{"cells":[{"cell_type":"code","source":["import sys,ast,os,time\nfrom cashflow_aua_aum import *\nfrom log import syslog,sysError\nfrom current_aua_aum import *\nfrom file_reader import *\nfrom staging_files import *\nfrom distribution_volumes import *\nfrom spark_conf import sparkConfig\nfrom transformations import *\nfrom baseline_data import *\nfrom staging_file_2_fbi_work import *\nfrom existingclientvetting import *\nfrom trc_export_final import *\nfrom copy_of_fsecpt_rollup import *\nfrom CET_Rollup import *\nfrom Adjustable_Data import *\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nfrom s3_connection import *\n\nparam = ast.literal_eval(sys.argv[1])\n# create spark session\nspark = sparkConfig(\"FSECPT\")\n\ndef main():\n    # try exception block to handle exceptions\n    try:\n        vcats_bucket = param['VCATS_S3_BUCKET']\n        crm_bucket = param['CRM_S3_BUCKET']\n        target_dir = param['TGT_S3_BUCKET']\n        ftp_bucket = param['FTP_S3_BUCKET']\n        target_path = param['DNI_S3_BUCKET']\n        conn = est_connection(target_path)\n\n        #create tables for input excel files\n        read_files(spark, ftp_bucket, target_dir + \"input_files/\")\n        #Methods for Cashflow AUA and AUM\n        df_plan_fnd_dim_filtered = iig_plnfnd_dim_import_job(crm_bucket,spark)\n        df_plan_dim = vcats_plan_dim_import_job(vcats_bucket,spark)\n        #workbook - CET_Cashflow_AUA, Sheet - Joined3\n        aua_df = cashflow_aua(vcats_bucket, df_plan_fnd_dim_filtered,spark,target_dir)\n        write_output_to_parquet(aua_df, target_dir,\"cashflow_aua\", \"overwrite\")\n\n        #workbook - CET_Cashflow_AUM, Sheet - Joined3\n        aum_df = cashflow_aum(vcats_bucket, df_plan_fnd_dim_filtered, spark,target_dir)\n        write_output_to_parquet(aum_df, target_dir,\"cashflow_aum\", \"overwrite\")\n\n        #workbook - CET_Current_VCATS, SheetS - Current_AUM, Current_AUA,DC_NQP_counts,Current_Plans,Current_Plans_v2\n        df_DC_NQP_Plan_count,df_current_aum,df_current_aua,df_current_clnt_plan,df_current_plans = cet_current_vcats(spark,df_plan_fnd_dim_filtered,df_plan_dim, vcats_bucket, target_dir)\n        write_output_to_parquet(df_current_plans, target_dir, \"current_plans\", \"overwrite\")\n        write_output_to_parquet(df_current_clnt_plan, target_dir, \"current_plans_v2\", \"overwrite\")\n        write_output_to_parquet(df_DC_NQP_Plan_count, target_dir, \"dc_nqp_counts\", \"overwrite\")\n        write_output_to_parquet(df_current_aum, target_dir, \"Current_AUM\", \"overwrite\")\n        write_output_to_parquet(df_current_aua, target_dir, \"Current_AUA\", \"overwrite\")\n\n        #workbook - DistributionVolumes_Staging_file, Sheet - Distribution_Volumes_2019\n        df_distribution_volumes = distribution_volumes(spark, df_plan_fnd_dim_filtered)\n        write_output_to_parquet(df_distribution_volumes, target_dir, \"distribution_volumes\", \"overwrite\")\n        #workbook - Expense_calc, Sheet - Ongoing_Expense_and_Margin\n        df_FSP_Revenue_Components = get_expense_calc(spark)\n        write_output_to_parquet(df_FSP_Revenue_Components, target_dir, \"ongoing_exp_margin\", \"overwrite\")\n        # workbook - Historic_Participant_Growth, Sheet - AverageGrowth, no data in test\n        #df_AvgGrowth = read_parquet(target_dir + \"hist_part_growth\", spark) # use it for testing in SAT\n        df_AvgGrowth = get_hist_part_growth(spark)\n        write_output_to_parquet(df_AvgGrowth, target_dir, \"hist_part_growth\", \"overwrite\")\n        # workbook - Staging File 0 Bill Code 447, Sheet - Fee_Disclosure\n        df_fee_disclosure = get_fee_disclosure(spark)\n        write_output_to_parquet(df_fee_disclosure, target_dir, \"stg_0_fee_disclosure\", \"overwrite\")\n        # workbook - Staging File 2_2 Ancil CATS for Baseline, Sheet - FSP_Ancil_cats\n        df_fsp_ancil_Cats = get_ancil_rev_Cats(spark)\n        write_output_to_parquet(df_fsp_ancil_Cats, target_dir, \"stg_2_2_ancil_cats\", \"overwrite\")\n        #workbook- staging File 1\n        df_add_vision_id,df_Add_Plan_Part_Count = add_plan_part_count(spark)\n        write_output_to_parquet(df_add_vision_id, target_dir, \"stg_1_add_vision_id\", \"overwrite\")\n        write_output_to_parquet(df_Add_Plan_Part_Count, target_dir, \"stg_1_add_plan_part_count\", \"overwrite\")\n        # workbook- baseline_data sheet - AUMandAUMadded,baseline_data,baseline_w_Part_Growth,baseline_w_Part_Growth_v2\n        df_curr_AUA_AUM,df_baseline_data,df_baseline_w_Part_Growth,df_baseline_w_Part_Growth_v2 = get_baseline_data(spark, df_AvgGrowth, df_fsp_ancil_Cats, target_dir)\n        write_output_to_parquet(df_curr_AUA_AUM, target_dir, \"AUMandAUMadded\", \"overwrite\")\n        write_output_to_parquet(df_baseline_data, target_dir, \"baseline_data\", \"overwrite\")\n        write_output_to_parquet(df_baseline_w_Part_Growth, target_dir, \"baseline_w_Part_Growth\", \"overwrite\")\n        write_output_to_parquet(df_baseline_w_Part_Growth_v2, target_dir, \"baseline_w_Part_Growth_v2\", \"overwrite\")\n        #workbook - staging file 2 fbi work continued\n        trc_work,seg_avg_edit = get_trc_work(spark,df_Add_Plan_Part_Count)\n        write_output_to_parquet(trc_work, target_dir, \"stg_2_trc_work\", \"overwrite\")\n        write_output_to_parquet(seg_avg_edit, target_dir, \"stg_2_seg_avg_edit\", \"overwrite\")\n        #workbook - compliance testing, sheet - Compliance_Testing_Volume\n        df_compliance_testing_volume = get_compliance_testing_volume(spark)\n        write_output_to_parquet(df_compliance_testing_volume, target_dir, \"compliance_testing_volume\", \"overwrite\")\n        # workbook - Staging File 2_1 Ancil Rev Categories, sheet - Ancil_Cats, Baseline_w_AncilRevCat\n        df_ancil_cats,df_baseline_ancil = get_ancil_cats(spark,df_baseline_data)\n        write_output_to_parquet(df_ancil_cats, target_dir, \"Ancil_Cats\", \"overwrite\")\n        write_output_to_parquet(df_baseline_ancil, target_dir, \"baseline_w_ancil\", \"overwrite\")\n        # workbook-staging file 8 trc work\n        df_percentile_rev_avg_bal, df_act_exp_add_seg, df_exp_driver = get_staging_file_8_trc_work(spark)\n        write_output_to_parquet(df_percentile_rev_avg_bal, target_dir, \"percentile_rev_avg_bal\", \"overwrite\")\n        write_output_to_parquet(df_act_exp_add_seg, target_dir, \"act_exp_add_seg\", \"overwrite\")\n        write_output_to_parquet(df_exp_driver, target_dir, \"exp_driver\", \"overwrite\")\n        # workbook - Baseline expense\n        df_total_expense, df_sheet22 = get_baseline_expense(spark)\n        write_output_to_parquet(df_total_expense, target_dir, \"baseline_expense_total_expense\", \"overwrite\")\n        write_output_to_parquet(df_sheet22, target_dir, \"baseline_expense_sheet22\", \"overwrite\")\n        # workbook - existing client vetting, sheet - cat vetting score,per_part_expense\n        df_cat_vetting_score = get_cat_vetting_score(spark, df_curr_AUA_AUM)\n        write_output_to_parquet(df_cat_vetting_score, target_dir, \"cat_vetting_score\", \"overwrite\")\n        df_per_part_exp = get_per_part_exp(spark)\n        write_output_to_parquet(df_per_part_exp, target_dir, \"per_part_expense\", \"overwrite\")\n        #workbook - staging file 3\n        df_client_rollup, df_add_baseline_data = get_add_baseline_data(spark,trc_work,df_baseline_ancil)\n        write_output_to_parquet(df_client_rollup, target_dir, \"staging_file_3_client_rollup\", \"overwrite\")\n        write_output_to_parquet(df_add_baseline_data, target_dir, \"add_baseline_data\", \"overwrite\")\n\n        # workbook - staging file 4 start of ancil recommendation\n        add_ancil_rev, asc_pricing = get_asc_pricing(spark, seg_avg_edit, df_add_baseline_data, df_ancil_cats)\n        write_output_to_parquet(add_ancil_rev, target_dir, \"staging_file_4_add_ancil_rev\", \"overwrite\")\n        write_output_to_parquet(asc_pricing, target_dir, \"staging_file_4_asc_pricing\", \"overwrite\")\n\n        # workbook - staging file 5\n        add_bill_volume_edit = get_add_bill_volume(spark, asc_pricing)\n        write_output_to_parquet(add_bill_volume_edit, target_dir, \"staging_file_5_add_bill_volume_edit\", \"overwrite\")\n\n        # workbook -staging file 6\n        DistributionEditV2, add_avg_billed = get_add_avg_billed(spark, add_bill_volume_edit, df_distribution_volumes, df_fee_disclosure, df_compliance_testing_volume)\n        write_output_to_parquet(DistributionEditV2, target_dir, \"staging_file_6_DistributionEditV2\", \"overwrite\")\n        write_output_to_parquet(add_avg_billed, target_dir, \"staging_file_6_add_avg_billed\", \"overwrite\")\n\n        #workbook - Staging File 7 Ancillary work, sheet - Ancil_work,Ancillary_Fee_Export. It's ideal to derive second data frame in dependent workbook\n        #df_Add_Avg_Billed = read_csv(ftp_bucket,\"test_input/Add_Avg_Billed.csv\",\",\",spark)\n        df_Ancil_work = get_stg_7_ancillary_work(spark,add_avg_billed)\n        write_output_to_parquet(df_Ancil_work, target_dir, \"Ancil_work\", \"overwrite\")\n\n        #workbook - staging file 9\n        blcl_df = read_csv(ftp_bucket,\"test_input/BLCL.csv\",\",\",spark)\n        # client_sens_rpt_df = read_csv(ftp_bucket,\"test_input/Client_Sensitivity_Rpt.csv\",\",\",spark)\n        client_sens_rpt_df = spark.sql(\n            \"\"\"with clnt_rank as (SELECT \n                van_companyid,van_ratingdate,van_ratingid,van_relationshipcategoryid_value,van_ratingtype,\n                RANK() OVER (partition by van_companyid ORDER BY van_ratingdate DESC) AS Rank\n                from inst_dna_compf_crm.relationshipratings\n                where van_relationshipcategoryid_value = 'Sensitivity / Rel. Status' and van_ratingtype = 'Full Service')\n            select distinct van_companyid,van_ratingdate,van_ratingid,van_relationshipcategoryid_value,van_ratingtype,Rank from clnt_rank where rank = 1\"\"\")\n        df_add_tenure = get_add_tenure(spark,df_act_exp_add_seg,blcl_df,add_ancil_rev,df_percentile_rev_avg_bal,client_sens_rpt_df)\n        write_output_to_parquet(df_add_tenure, target_dir, \"staging_file_9_add_tenure\", \"overwrite\")\n\n        # workbook - Staging File 10 REC work cont, sheet - Add_Non_Act_uplift\n        #df_add_tenure = read_csv(ftp_bucket,\"test_input/Add_Tenure.csv\",\",\",spark)\n        df_Non_Act_uplift = get_Non_Act_uplift(spark, df_Ancil_work, df_add_tenure, df_baseline_w_Part_Growth,ftp_bucket)\n        write_output_to_parquet(df_Non_Act_uplift, target_dir, \"Non_Act_uplift\", \"overwrite\")\n\n        # workbook - Staging File 11 REC work cont, sheet - TRC_Export_final\n        df_target_margins = read_csv(ftp_bucket, \"test_input/FSP_Target_Margins.csv\", \",\", spark)\n        df_get_TRC_Export_final,df_get_TRC_Export = get_TRC_Export_final(spark, df_add_vision_id, df_Non_Act_uplift, df_target_margins,df_exp_driver)\n        write_output_to_parquet(df_get_TRC_Export_final, target_dir, \"TRC_Export_final\", \"overwrite\")\n        write_output_to_parquet(df_get_TRC_Export, target_dir, \"TRC_Export\", \"overwrite\")\n\n        #workbook -staging file 12\n        # df_get_TRC_Export_final = read_csv(ftp_bucket,\"test_input/TRC_Export_Final_Copy.csv\")\n        TRC_Export_V2,ancil_n_dist,final_ancillary_export = get_trc_export_v2(spark, df_get_TRC_Export_final, DistributionEditV2, df_Ancil_work,df_current_plans)\n        write_output_to_parquet(TRC_Export_V2, target_dir, \"staging_file_12_TRC_Export_V2\", \"overwrite\")\n        write_output_to_parquet(ancil_n_dist, target_dir, \"staging_file_12_ancil_n_dist\", \"overwrite\")\n        write_output_to_parquet(final_ancillary_export, target_dir, \"staging_file_12_final_ancillary_export\", \"overwrite\")\n\n        #workbook- staging file 13\n        # ancil_n_dist = read_csv(ftp_bucket,\"test_input/Ancil_n_Dist_Copy.csv\",\",\",spark)\n        AncilCats = get_AncilCats(spark,df_ancil_cats,ancil_n_dist)\n        write_output_to_parquet(AncilCats, target_dir, \"staging_file_13_AncilCats\", \"overwrite\")\n\n        # workbook-CET Rollup\n        #Final_Group_CF_AUA = read_csv(ftp_bucket, \"test_input/Final_Group_CF_AUA_copy.csv\", \",\", spark)\n        Tenure = read_csv(ftp_bucket, \"test_input/Tenure_copy.csv\", \",\", spark)\n        # df_cat_vetting_score = read_csv(ftp_bucket, \"test_input/cat_vetting_score_copy.csv\", \",\", spark) # use it for testing in SAT\n        Rollup = get_Rollup(spark, df_client_rollup, df_baseline_w_Part_Growth, aum_df, df_AvgGrowth,aua_df, Tenure, df_FSP_Revenue_Components,ancil_n_dist, df_get_TRC_Export, df_cat_vetting_score, df_DC_NQP_Plan_count, df_total_expense,df_current_clnt_plan)\n        write_output_to_parquet(Rollup, target_dir, \"CET_Rollup\", \"overwrite\")\n\n        #workbook- copy of FSECPT Sheet # Sheet5\n        BLCL = read_csv(ftp_bucket,\"test_input/BLCL.csv\",\",\",spark)\n        # Roll_Up = read_csv(ftp_bucket,\"test_input/RollUp_copy.csv\",\",\",spark)\n        Joined4 = get_copy_fsecpt_rollup(spark,df_per_part_exp,BLCL,Rollup,AncilCats,client_sens_rpt_df)\n        write_output_to_parquet(Joined4, target_path, \"bsln_adj_recc\",\"overwrite\")\n\n        #workbook - Adjustable data\n        sheet4 = read_csv(ftp_bucket,\"test_input/Adjustable_data_sheet4_copy.csv\",\",\",spark)\n        Ancil_Fee,sheet1 = get_adjustable_data(spark,sheet4,final_ancillary_export,df_baseline_w_Part_Growth_v2,df_get_TRC_Export_final,df_sheet22,AncilCats)\n        clnt_fee_df = Ancil_Fee.toPandas()\n        clnt_fee_table = pa.Table.from_pandas(clnt_fee_df, preserve_index=False)\n        clnt_demo_df = sheet1.toPandas()\n        clnt_demo_table = pa.Table.from_pandas(clnt_demo_df, preserve_index=False)\n        pq.write_table(clnt_demo_table, target_path + \"fcpt_clnt_demo/clnt_demo.parquet\",flavor='spark', filesystem=conn)\n        pq.write_table(clnt_fee_table, target_path + \"fcpt_clnt_fee/clnt_fee.parquet\",flavor='spark', filesystem=conn)\n        # write_output_to_parquet(sheet1, target_path, \"fcpt_clnt_demo\", \"overwrite\")\n        # write_output_to_parquet(Ancil_Fee, target_path, \"fcpt_clnt_fee\", \"overwrite\")\n        # spark.stop()\n        # time.sleep(60)\n        # demo_target_path = target_path+\"fcpt_clnt_demo\"\n        # os.system(\"hadoop distcp {}/part*  {}.parquet\".format(demo_target_path, demo_target_path + \"/clnt_demo\"))\n        # os.system(\"hadoop fs -rm -r {}/part*\".format(demo_target_path))\n        #\n        # ancil_target_path = target_path + \"fcpt_clnt_fee\"\n        # os.system(\"hadoop distcp {}/part*  {}.parquet\".format(ancil_target_path, ancil_target_path + \"/clnt_fee\"))\n        # os.system(\"hadoop fs -rm -r {}/part*\".format(ancil_target_path))\n\n    except Exception as e:\n        sysError('Method :\\'{}\\' failed with exception : '.format(main.__name__), e)\n\nif __name__ == '__main__':\n    main()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4fd78058-ae82-45db-b6c1-5cc61faff973"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"etl","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2615474751383702}},"nbformat":4,"nbformat_minor":0}
