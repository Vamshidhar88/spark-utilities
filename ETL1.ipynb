{"cells":[{"cell_type":"code","source":["import json\nimport ast\nimport sys\nfrom aurora_config import *\nfrom log import syslog, sysError\nfrom spark_conf import sparkConfig\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.functions import *\n\ndef main():\n  #reading the input params from configuration file\n  spark = sparkConfig()\n\n  input_param = ast.literal_eval(sys.argv[1])\n  hostname=input_param['ENV']\n  outputbucket=input_param['S3BUCKET']\n  tableconfig=input_param['INGESTIONFILE']\n  driver=input_param['DRIVER']\n  secrets=get_username_password(hostname,tableconfig)\n  username=secrets[0]\n  p=secrets[2]\n  jdbc_url=secrets[1]\n  connectionProperties = {\n    \"user\" : username,\n    \"password\" : p,\n    \"driver\":driver\n  }\n\n  #reading the tables\n  wc_vcats_data_e = spark.read.jdbc(url=jdbc_url, table='wc_vcats_data_e', properties=connectionProperties)\n  wc_vcats_data_e = wc_vcats_data_e.select(col(\"*\"), when(wc_vcats_data_e.plan_sub_type == \"Fine Arts/Specialized Organiza\", \"Fine Arts/Specialized Organization\").otherwise(\n    wc_vcats_data_e.plan_sub_type).alias(\"plan_sub_type1\")).drop(col(\"plan_sub_type\")).withColumnRenamed(\"plan_sub_type1\",\"plan_sub_type\")\n  wc_vcats_data_busln = spark.read.jdbc(url=jdbc_url, table='wc_vcats_data_busln', properties=connectionProperties)\n  wc_vcats_data_n = spark.read.jdbc(url=jdbc_url, table='wc_vcats_data_n', properties=connectionProperties)\n\n  #This function uses to process the fact tables by taking parameters  as table name and column from the configuration file\n  def fact_table(table, col1):\n    try:\n      df_wc_vcats_data_fact = spark.read.jdbc(url=jdbc_url, table=table, properties=connectionProperties)\n      if table == \"wc_vcats_data_e\":\n        df_wc_vcats_data_fact = df_wc_vcats_data_fact.select(col(\"*\"), when(df_wc_vcats_data_fact.plan_sub_type == \"Fine Arts/Specialized Organiza\", \"Fine Arts/Specialized Organization\").otherwise(\n                                df_wc_vcats_data_fact.plan_sub_type).alias(\"plan_sub_type1\")).drop(col(\"plan_sub_type\")).withColumnRenamed(\"plan_sub_type1\",\"plan_sub_type\")\n\n      bus_line = [\"Institutional Investor Group\", \"FAS\"]\n      df_wc_vcats_data_fact_filtered = wc_vcats_data_busln.select(\"dim_key\").distinct().where(col(\"bus_line_roll_up1\").isin(bus_line))\n      df_data_e_filtered = df_wc_vcats_data_fact.join(df_wc_vcats_data_fact_filtered, df_wc_vcats_data_fact[col1] == df_wc_vcats_data_fact_filtered[\"dim_key\"], \"inner\").drop(\"dim_key\")\n      df_fact = df_data_e_filtered.coalesce(1).write.mode(\"overwrite\").option(\"path\", outputbucket + \"/\" + table).saveAsTable(\n        \"inst_dni_fin_mstr.\" + table)\n      syslog('Total Number of records got ingested for {0}={1}'.format(table, df_data_e_filtered.count()))\n      return df_fact\n    except Exception as ex:\n      sysError('Unable to ingest the data for Fact table ', ex)\n\n  #This function uses to process the Dimension tables by taking parameters as table name and column names from the configuration file\n  def dimensions_table(table, col1, col2):\n    try:\n      df_wc_vcats_dimension = spark.read.jdbc(url=jdbc_url, table=table, properties=connectionProperties)\n      bus_line = [\"Institutional Investor Group\", \"FAS\"]\n      df_wc_vcats_data_busln_filtered = wc_vcats_data_busln.select(\"dim_key\").distinct().where(col('bus_line_roll_up1').isin(bus_line))\n      df_data_e_filtered = wc_vcats_data_e.join(df_wc_vcats_data_busln_filtered,\n                                                  wc_vcats_data_e[\"business_sub_line\"] ==\n                                                  df_wc_vcats_data_busln_filtered[\"dim_key\"], \"inner\").select(col2)\n      df_data_n_filtered = wc_vcats_data_n.join(df_wc_vcats_data_busln_filtered,\n                                                  wc_vcats_data_n[\"business_sub_line\"] ==\n                                                  df_wc_vcats_data_busln_filtered[\"dim_key\"], \"inner\").select(col2)\n      df_data_dimension_filtered = df_data_e_filtered.union(df_data_n_filtered).distinct()\n      df_data_dimension_finaljoin = df_wc_vcats_dimension.join(df_data_dimension_filtered,\n                                                            df_wc_vcats_dimension[col1] == df_data_dimension_filtered[\n                                                              col2], \"inner\").drop(df_data_dimension_filtered[col2])\n      df_dimension = df_data_dimension_finaljoin.coalesce(1).write.mode('overwrite').option(\"path\",outputbucket+\"/\"+table).saveAsTable(\"inst_dni_fin_mstr.\"+table)\n      syslog('Total Number of records got ingested for  {0}={1}'.format(table,df_data_dimension_finaljoin.count()))\n      return df_dimension\n    except Exception as ex:\n      sysError('Unable to ingest the data for Dimension table ', ex)\n\n  # This function uses to process the wc_vcats_data_busln table by taking parameters as table name and column names from the configuration file\n  def dwc_vcats_data_busln(table, col1):\n    try:\n      data_busln = spark.read.jdbc(url=jdbc_url, table=table, properties=connectionProperties)\n      bus_line = [\"Institutional Investor Group\", \"FAS\"]\n      df = data_busln.where(col(col1).isin(bus_line))\n      df_busln = df.coalesce(1).write.mode('overwrite').option(\"path\",outputbucket+\"/\"+table).saveAsTable(\"inst_dni_fin_mstr.\"+table)\n      syslog('Total Number of records got ingested for {0}={1}'.format(table,df.count()))\n      return df_busln\n    except Exception as ex:\n      sysError('Unable to ingest the data for Businessline table ', ex)\n # this function is used to process the wc_vcats_fund_ds table\n  def dwc_vcats_fund_ds(table):\n    try:\n      fund_fs=spark.read.jdbc(url=jdbc_url, table=table, properties=connectionProperties)\n      final_fund_fs=fund_fs.coalesce(1).write.mode('overwrite').option(\"path\",outputbucket+\"/\"+table).saveAsTable(\"inst_dni_fin_mstr.\"+table)\n      syslog('Total Number of records got ingested for   {0}={1}'.format(table, fund_fs.count()))\n      return final_fund_fs\n    except Exception as ex:\n      sysError('Unable to ingest the data for fund table ', ex)\n# this function is used to process the wc_vcats_cashflow_f and wc_vcats_siebel_pln_ds to get the vcats_plan_company_name table\n  def vcats_plan_company_name(table1,table2):\n    try:\n      df_wc_vcats_cashflow_f=spark.read.jdbc(url=jdbc_url, table=table1, properties=connectionProperties)\n      df_wc_vcats_siebel_pln_ds = spark.read.jdbc(url=jdbc_url, table=table2, properties=connectionProperties)\n      df_wc_vcats_cashflow_startdate = df_wc_vcats_cashflow_f.withColumn(\"Date\", trunc(\"AS_OF_DATE\", \"month\"))\n      df_wc_vcats_cashflow_col = (df_wc_vcats_cashflow_startdate.where(col(\"PLAN_NUM_WO_PREFIX\") != \"52686\").\n                                  select(col(\"Date\"), col(\"PLAN_NUM_WO_PREFIX\").alias(\"Plan__\")))\n      df_wc_vcats_siebel_pln_ds_col = (\n        df_wc_vcats_siebel_pln_ds.where(col(\"BUS_LINE\").isin(\"IAM\", \"IRPS\", \"Small Markets\")).\n        select(col(\"PLAN_NUM\"), col(\"BUS_LINE\").alias(\"Business_Line\"), col(\"CO_NAME\").alias(\"Company_Name\"),\n               col(\"PLAN_NAME\").alias(\"Plan_Name\")))\n      df_joined = df_wc_vcats_cashflow_col.join(df_wc_vcats_siebel_pln_ds_col,df_wc_vcats_cashflow_col.Plan__ == df_wc_vcats_siebel_pln_ds_col.PLAN_NUM).drop(\"PLAN_NUM\")\n      df_distinct=df_joined.distinct()\n      df_vcats_plan_company_name = df_distinct.coalesce(1).write.mode(\"overwrite\").option(\"path\",outputbucket+\"/\"+\"vcats_plan_company_name\").saveAsTable(\"inst_dni_fin_mstr.\"+\"vcats_plan_company_name\")\n      syslog('Total Number of records got ingested for  {0}={1}'.format(\"vcats_Plan_Company_Name\", df_distinct.count()))\n      return df_vcats_plan_company_name\n\n    except Exception as ex:\n      sysError('Unable to ingest the data for vcats_plan_company_name table ', ex)\n\n  # below code is used for parsing configuration json file to get the table names and column names\n  with open(tableconfig) as json_file:\n    data = json.load(json_file)\n    table_list = data['tables']\n\n  tabdict = {}\n  for table in table_list:\n    for key, value in table.items():\n      tabdict[key] = value\n  table_names = list(tabdict.keys())\n\n  for i in range(len(table_names)):\n    tablename = table_names[i]\n    if tablename in tabdict.keys() and tabdict[tablename]['type'] == \"fact\":\n      table = tabdict[tablename]['table']\n      col1 = tabdict[tablename]['column'][0]\n      fact_table(table, col1)\n    elif tablename in tabdict.keys() and tabdict[tablename]['type'] == \"Dimension\" and tablename != \"wc_vcats_data_busln\" and tablename !=\"wc_vcats_fund_ds\":\n      table = tabdict[tablename]['table']\n      col1 = tabdict[tablename]['column'][0]\n      col2 = tabdict[tablename]['column'][1]\n      dimensions_table(table, col1, col2)\n    elif tablename in tabdict.keys() and tabdict[tablename]['type'] == \"Dimension\" and tablename == \"wc_vcats_data_busln\":\n      table = tabdict[tablename]['table']\n      col1 = tabdict[tablename]['column']\n      dwc_vcats_data_busln(table, col1)\n    elif tablename in tabdict.keys() and tabdict[tablename]['type']==\"Dimension\" and tablename==\"wc_vcats_fund_ds\":\n      table=tabdict[tablename]['table']\n      dwc_vcats_fund_ds(table)\n    elif tablename in tabdict.keys() and tabdict[tablename]['type']=='Derived':\n      table1 = tabdict[tablename]['table'][0]\n      table2 = tabdict[tablename]['table'][1]\n      vcats_plan_company_name(table1,table2)\n    else:\n      print(\"enter the correct table name\")\n    i += 1\n\nif __name__ == '__main__':\n    main()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4856cbc2-87c7-42d6-8bfa-5e4916215cc0"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ETL","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3995740736622367}},"nbformat":4,"nbformat_minor":0}
