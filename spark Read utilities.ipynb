{"cells":[{"cell_type":"code","source":["from log import *\nfrom pyspark.sql.types import *\nimport pandas as pd\n\n\ndef read_csv(source_dir,file_name,delimiter,sparksession):\n    try:\n        df_file_input = sparksession.read \\\n            .format('com.databricks.spark.csv') \\\n            .options(header='true', delimiter=delimiter) \\\n            .load(source_dir + file_name)\n        syslog('Successfully read data from file : {}'.format(source_dir+file_name))\n        return df_file_input\n    except Exception as ex:\n        sysError('Failed to read from input csv file :{}'.format(source_dir+file_name), ex)\n\ndef read_multicsv(file_list,delimiter,sparksession):\n    try:\n        df_file_input = sparksession.read \\\n            .format('com.databricks.spark.csv') \\\n            .options(header='true', delimiter=delimiter) \\\n            .load(file_list)\n        syslog('Successfully read data from files : {}'.format(file_list))\n        return df_file_input\n    except Exception as ex:\n        sysError('Failed to read from csv files :{}'.format(file_list), ex)\n\n#read parquet file\ndef read_parquet(source_dir,sparksession):\n    try:\n        df_file_input = sparksession.read.parquet(source_dir)\n        syslog('Successfully read data from parquet folder : {}'.format(source_dir))\n        return df_file_input\n    except Exception as ex:\n        sysError('Failed to read from input parquet folder :{}'.format(source_dir), ex)\n\n#read data from partitioned parquet file\ndef read_parquet_partition(source_dir,sparksession,partition):\n    try:\n        df_file_input = sparksession.read.parquet(source_dir+\"/\"+partition)\n        syslog('Successfully read data from parquet folder : {}'.format(source_dir))\n        return df_file_input\n    except Exception as ex:\n        sysError('Failed to read from input parquet folder :{}'.format(source_dir), ex)\n\n\n#read excel data in pyspark dataframe\ndef read_excel_file_withoutSheetName(spark,input_bucket, file_name):\n    try:\n        file_path=input_bucket + file_name\n        df_spark_df = pandas_to_spark((pd.read_excel(file_path, engine='openpyxl')), spark)\n        syslog('Successfully read data from excel file : {}'.format(file_path))\n        return df_spark_df\n    except Exception as ex:\n        sysError('Failed to read from input excel file :{}'.format(file_path), ex)\n\ndef read_excel_file_withoutSheetName_fillna(spark,input_bucket, file_name):\n    try:\n        file_path=input_bucket + file_name\n        df_spark_df = pandas_to_spark((pd.read_excel(file_path, engine='openpyxl').fillna('')), spark)\n        syslog('Successfully read data from excel file : {}'.format(file_path))\n        return df_spark_df\n    except Exception as ex:\n        sysError('Failed to read from input excel file :{}'.format(file_path), ex)\n\n#read excel data in pyspark dataframe\ndef read_excel_withfillna(spark,input_bucket, file_name, sheet_name):\n    try:\n        file_path=input_bucket + file_name\n        df_spark_df = pandas_to_spark((pd.read_excel(file_path, sheet_name=sheet_name,\n                                        engine='openpyxl').fillna('')), spark)\n\n        syslog('Successfully read data from excel file : {}'.format(file_path))\n        return df_spark_df\n    except Exception as ex:\n        sysError('Failed to read from input excel file :{}'.format(file_path), ex)\n\n#Updated version of read_excel_file method which reduces number of input parameters\ndef read_excel(sparksession, input_bucket, file_name, sheet_name):\n    try:\n        source_dir = input_bucket + file_name\n        df_spark_df = pandas_to_spark(pd.read_excel(input_bucket + file_name, sheet_name=sheet_name,engine='openpyxl'), sparksession)\n        syslog('Successfully read data from source folder : {}'.format(source_dir))\n        return df_spark_df\n    except Exception as ex:\n        sysError('Failed to read from input source folder :{}'.format(source_dir), ex)\n\ndef pandas_to_spark(pandas_df,spark):\n    columns = list(pandas_df.columns)\n    print(columns)\n    types = list(pandas_df.dtypes)\n    struct_list = []\n    for column, typo in zip(columns, types):\n        struct_list.append(define_structure(column, typo))\n    p_schema = StructType(struct_list)\n    return spark.createDataFrame(pandas_df,p_schema )\n\ndef define_structure(string, format_type):\n    try:\n        typo = equivalent_type(format_type)\n    except:\n        typo = StringType()\n    return StructField(string, typo)\n\n# Auxiliar functions\ndef equivalent_type(f):\n    if f == 'datetime64[ns]':\n        return TimestampType()\n    elif f == 'int64':\n        return LongType()\n    elif f == 'int32':\n        return IntegerType()\n    elif f == 'float64':\n        return FloatType()\n    else:\n        return StringType()\n\ndef read_excel_wtalrt(sparksession, input_bucket, file_name, sheet_name,alert_var):\n    try:\n        source_dir = input_bucket + file_name\n        df_spark_df = pandas_to_spark(pd.read_excel(input_bucket + file_name, sheet_name=sheet_name,engine='openpyxl'), sparksession)\n        syslog('Successfully read data from source folder : {}'.format(source_dir))\n        return df_spark_df\n    except Exception as ex:\n        syserror_alert(\"Failed to read from input source folder\", ex, alert_var)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e271a7cb-301b-433b-a2e3-bd5c9be417bc"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"spark Read utilities","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2615474751383700}},"nbformat":4,"nbformat_minor":0}
